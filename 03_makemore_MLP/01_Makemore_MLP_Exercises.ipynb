{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference Paper: [A Neural Probabilistic Language Model - Bengio et.al.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        "<br>\n",
        "## Graphic of an MLP:\n",
        "![MLP](https://i.postimg.cc/4d03m46Q/Screenshot-2025-06-09-022351.png)"
      ],
      "metadata": {
        "id": "5hh-a2qeKJ-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "aFocNqGNM8MC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Get a loss < 2.2 (Parameter & Hyperparameter tuning)"
      ],
      "metadata": {
        "id": "hZDfgy3qcC9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset"
      ],
      "metadata": {
        "id": "ZYf4pT_FLhFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQWA4D3kKD5E"
      },
      "outputs": [],
      "source": [
        "# Collecting Words\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/AndrejKarpathy_NN_Hero/names.txt'\n",
        "words = open(path, 'r').read().splitlines()\n",
        "\n",
        "# Mapping\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 4 # number of characters as input\n",
        "X, Y = [], []\n",
        "\n",
        "for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w+'.':\n",
        "        ix = stoi[ch]\n",
        "        X.append(context)\n",
        "        Y.append(ix)\n",
        "        context = context[1:] + [ix]\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "# Split the dataset (train: 80%, dev: 10%, test:10%)\n",
        "rs = 2147483647\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.8, random_state=rs, shuffle=True)\n",
        "X_dev, X_test, Y_dev, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, random_state=rs, shuffle=True)"
      ],
      "metadata": {
        "id": "m5i0U8RkLgIp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating MLP Network"
      ],
      "metadata": {
        "id": "lkFpXbptPQ8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "n_emb = 4 # number of embedding layers\n",
        "n_h = 200 # number of Neurons in hidden layer\n",
        "\n",
        "C = torch.randn((27, n_emb), generator=g, requires_grad=True) # Embedding in Lookup layer\n",
        "w1 = torch.randn(( n_emb * block_size, n_h), generator=g, requires_grad=True) # hidden layer weights\n",
        "b1 = torch.randn((n_h), generator=g, requires_grad=True) # hidden layer biases\n",
        "w2 = torch.randn((n_h, 27), generator=g, requires_grad=True) # Last layer (output)\n",
        "b2 = torch.randn((27), generator=g, requires_grad=True) # output layer biases\n",
        "\n",
        "params = [C, w1, b1, w2, b2]"
      ],
      "metadata": {
        "id": "X3BDP9v2NaFv"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total parameter count\n",
        "sum(p.nelement() for p in params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU3Cw4M9OGRQ",
        "outputId": "dca6c341-0688-4573-916c-29d87407c683"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8935"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning"
      ],
      "metadata": {
        "id": "Jx5acBdqcJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "itrns = 200000\n",
        "\n",
        "# learning rates\n",
        "lri = torch.linspace(0, -3, itrns) # from 0 to -3 with itrns number of steps\n",
        "lre = 10**lri # from 10*0=1 to 10**-3=0.001 in itrns number of steps\n",
        "\n",
        "for i in range(itrns):\n",
        "    # minibatch\n",
        "    ix = torch.randint(low=0, high=X_train.shape[0], size=(64, ), generator=g)\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[X_train[ix]]\n",
        "    h = torch.tanh((emb.view(-1, block_size * n_emb) @ w1 + b1))\n",
        "    logits = h @ w2 + b2\n",
        "    loss = F.cross_entropy(logits, Y_train[ix])\n",
        "\n",
        "    # backward pass\n",
        "    for p in params:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # nudge\n",
        "    lr = lre[i]\n",
        "    for p in params:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    if (i*1.0) % 10000 == 0.0:\n",
        "        print(f'Loss on iteration {i}: {loss.item()} | Learning Rate: {lr}')\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYsaw9ogSea3",
        "outputId": "b96311b5-da60-4017-953f-952710ff88dd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on iteration 0: 25.550077438354492 | Learning Rate: 1.0\n",
            "Loss on iteration 10000: 2.64593243598938 | Learning Rate: 0.7079445719718933\n",
            "Loss on iteration 20000: 2.6186728477478027 | Learning Rate: 0.5011854767799377\n",
            "Loss on iteration 30000: 2.4271085262298584 | Learning Rate: 0.35481154918670654\n",
            "Loss on iteration 40000: 2.0755672454833984 | Learning Rate: 0.25118690729141235\n",
            "Loss on iteration 50000: 2.2218549251556396 | Learning Rate: 0.1778264045715332\n",
            "Loss on iteration 60000: 2.1787593364715576 | Learning Rate: 0.12589123845100403\n",
            "Loss on iteration 70000: 1.892807126045227 | Learning Rate: 0.08912400156259537\n",
            "Loss on iteration 80000: 2.1421315670013428 | Learning Rate: 0.063094861805439\n",
            "Loss on iteration 90000: 2.2556822299957275 | Learning Rate: 0.04466765746474266\n",
            "Loss on iteration 100000: 2.213229179382324 | Learning Rate: 0.031622231006622314\n",
            "Loss on iteration 110000: 1.9161920547485352 | Learning Rate: 0.022386789321899414\n",
            "Loss on iteration 120000: 2.223454475402832 | Learning Rate: 0.01584860309958458\n",
            "Loss on iteration 130000: 1.8616225719451904 | Learning Rate: 0.01121993362903595\n",
            "Loss on iteration 140000: 2.072364568710327 | Learning Rate: 0.007943091914057732\n",
            "Loss on iteration 150000: 2.087120771408081 | Learning Rate: 0.005623267963528633\n",
            "Loss on iteration 160000: 1.7968692779541016 | Learning Rate: 0.003980961628258228\n",
            "Loss on iteration 170000: 2.126178741455078 | Learning Rate: 0.002818299690261483\n",
            "Loss on iteration 180000: 1.9442448616027832 | Learning Rate: 0.0019952007569372654\n",
            "Loss on iteration 190000: 2.1484837532043457 | Learning Rate: 0.00141249131411314\n",
            "2.1126108169555664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss for Dev dataset\n",
        "\n",
        "emb = C[X_dev]\n",
        "h = torch.tanh(emb.view(-1, block_size * n_emb) @ w1 + b1)\n",
        "logits = h @ w2 + b2\n",
        "loss = F.cross_entropy(logits, Y_dev)\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rf4BVA8S8kX",
        "outputId": "42718a10-d4e7-4be1-c1ef-a6e4284cc807"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1384639739990234"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best loss achieved with 100 Neurons = 2.20\n",
        "# Let's increase number of neurons to 200, and keep everything else same:\n",
        "\n",
        "# Other methods to optimize:\n",
        "    # best way is to increase the number of Neurons in the hidden layer (100 -> 300)\n",
        "    # or we can also increase the layers in our embedding (2 -> 10)\n",
        "    # or we can change the number of characters we are feeding (context 3 -> 4, 5)\n",
        "    # or we can run more training loops with decaying learning rate\n",
        "    # or we can change the batch size (32 -> 64)"
      ],
      "metadata": {
        "id": "ano7VZqzWhFy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best loss achieved with 200 Neurons = 2.13846"
      ],
      "metadata": {
        "id": "tWALwj4HbN3M"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss for Test dataset\n",
        "\n",
        "emb = C[X_test]\n",
        "h = torch.tanh(emb.view(-1, block_size * n_emb) @ w1 + b1)\n",
        "logits = h @ w2 + b2\n",
        "loss = F.cross_entropy(logits, Y_test)\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYooQWXVbSQc",
        "outputId": "495b5cd2-443f-4cb2-de89-2fccd39b5d85"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1387529373168945"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our model shows consistent loss for all three datasets, which means it has not been overfit to the training data.\n",
        "# Let's stick to it now"
      ],
      "metadata": {
        "id": "f_32bf5pbWPn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLING\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size\n",
        "\n",
        "    while True:\n",
        "        emb = C[torch.tensor([context])]\n",
        "        h = torch.tanh(emb.view(1, -1) @ w1 + b1)\n",
        "        logits = h @ w2 + b2\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "        context = context[1:] + [ix]\n",
        "        if ix ==0:\n",
        "            break\n",
        "        out.append(ix)\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRY6ILIlbf_q",
        "outputId": "b62e1ef4-26ef-45ae-c1ba-aa861a91a875"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "morachmyah\n",
            "seelv\n",
            "dhryah\n",
            "rener\n",
            "jendrie\n",
            "caden\n",
            "daelin\n",
            "shivoel\n",
            "edeliennanar\n",
            "kayzim\n",
            "mara\n",
            "noshur\n",
            "rishianna\n",
            "kinton\n",
            "kynlee\n",
            "pair\n",
            "ubemavd\n",
            "ryyah\n",
            "faeha\n",
            "kayshawn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Very much name like outputs.\n",
        "# Goal achieved: Get loss less than 2.2, got 2.13"
      ],
      "metadata": {
        "id": "O-Q9BMwSbrzY"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: Initializing with close to uniform weights\n",
        "\n",
        "We can make the weights smaller to make them more uniform (closer)\n",
        "Multiply by 0.01\n",
        "\n",
        "<u>Make sure to call requires_grad after the multiplication as any operation will remove grads from non-leaf tensors<u>"
      ],
      "metadata": {
        "id": "yW2KbjqCdZiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "n_emb = 4 # number of embedding layers\n",
        "n_h = 200 # number of Neurons in hidden layer\n",
        "\n",
        "Cx = (torch.randn((27, n_emb), generator=g) * 0.01).requires_grad_()\n",
        "w1x = (torch.randn(( n_emb * block_size, n_h), generator=g) * 0.01).requires_grad_()\n",
        "b1x = torch.zeros((n_h), requires_grad=True)\n",
        "w2x = (torch.randn((n_h, 27), generator=g) * 0.01).requires_grad_()\n",
        "b2x = torch.zeros((27), requires_grad=True)\n",
        "\n",
        "paramsx = [Cx, w1x, b1x, w2x, b2x]"
      ],
      "metadata": {
        "id": "SOhePsAFdog9"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "itrns = 200000\n",
        "\n",
        "# learning rates\n",
        "lri = torch.linspace(0, -3, itrns) # from 0 to -3 with itrns number of steps\n",
        "lre = 10**lri # from 10*0=1 to 10**-3=0.001 in itrns number of steps\n",
        "\n",
        "for i in range(itrns):\n",
        "    # minibatch\n",
        "    ix = torch.randint(low=0, high=X_train.shape[0], size=(64, ), generator=g)\n",
        "\n",
        "    # forward pass\n",
        "    emb = Cx[X_train[ix]]\n",
        "    h = torch.tanh((emb.view(-1, block_size * n_emb) @ w1x + b1x))\n",
        "    logits = h @ w2x + b2x\n",
        "    loss = F.cross_entropy(logits, Y_train[ix])\n",
        "\n",
        "    # backward pass\n",
        "    for p in paramsx:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    for j, p in enumerate(paramsx):\n",
        "        if p.grad is None:\n",
        "            print(f\"Gradient is None for parameter {j} (name: {['Cx', 'w1x', 'b1x', 'w2x', 'b2x'][j]}) at iteration {i}\")\n",
        "    # --- End of added diagnostic code ---\n",
        "\n",
        "    # nudge\n",
        "    lr = lre[i]\n",
        "    for p in paramsx:\n",
        "        p.data += -lr * p.grad\n",
        "\n",
        "    if (i*1.0) % 10000 == 0.0:\n",
        "        print(f'Loss on iteration {i}: {loss.item()} | Learning Rate: {lr}')\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LybaIhFIeAxX",
        "outputId": "927fdfa2-f76b-4975-cb01-29202091a8ca"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on iteration 0: 3.295844316482544 | Learning Rate: 1.0\n",
            "Loss on iteration 10000: 2.3734588623046875 | Learning Rate: 0.7079445719718933\n",
            "Loss on iteration 20000: 2.461765766143799 | Learning Rate: 0.5011854767799377\n",
            "Loss on iteration 30000: 2.245591878890991 | Learning Rate: 0.35481154918670654\n",
            "Loss on iteration 40000: 2.2268147468566895 | Learning Rate: 0.25118690729141235\n",
            "Loss on iteration 50000: 2.2551960945129395 | Learning Rate: 0.1778264045715332\n",
            "Loss on iteration 60000: 2.383298397064209 | Learning Rate: 0.12589123845100403\n",
            "Loss on iteration 70000: 2.127276659011841 | Learning Rate: 0.08912400156259537\n",
            "Loss on iteration 80000: 2.41489839553833 | Learning Rate: 0.063094861805439\n",
            "Loss on iteration 90000: 2.2968294620513916 | Learning Rate: 0.04466765746474266\n",
            "Loss on iteration 100000: 2.205369710922241 | Learning Rate: 0.031622231006622314\n",
            "Loss on iteration 110000: 2.113295793533325 | Learning Rate: 0.022386789321899414\n",
            "Loss on iteration 120000: 2.336944341659546 | Learning Rate: 0.01584860309958458\n",
            "Loss on iteration 130000: 2.033811092376709 | Learning Rate: 0.01121993362903595\n",
            "Loss on iteration 140000: 2.313915729522705 | Learning Rate: 0.007943091914057732\n",
            "Loss on iteration 150000: 2.1271004676818848 | Learning Rate: 0.005623267963528633\n",
            "Loss on iteration 160000: 1.9386584758758545 | Learning Rate: 0.003980961628258228\n",
            "Loss on iteration 170000: 2.341282606124878 | Learning Rate: 0.002818299690261483\n",
            "Loss on iteration 180000: 2.114840269088745 | Learning Rate: 0.0019952007569372654\n",
            "Loss on iteration 190000: 2.1919002532958984 | Learning Rate: 0.00141249131411314\n",
            "2.172150135040283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss for Dev dataset\n",
        "\n",
        "emb = Cx[X_dev]\n",
        "h = torch.tanh(emb.view(-1, block_size * n_emb) @ w1x + b1x)\n",
        "logits = h @ w2x + b2x\n",
        "loss = F.cross_entropy(logits, Y_dev)\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fgx9-YtfN-w",
        "outputId": "bf3a7c80-83c3-48b3-e855-352f123da53b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1891355514526367"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss for Test dataset\n",
        "\n",
        "emb = Cx[X_test]\n",
        "h = torch.tanh(emb.view(-1, block_size * n_emb) @ w1x + b1x)\n",
        "logits = h @ w2x + b2x\n",
        "loss = F.cross_entropy(logits, Y_test)\n",
        "loss.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj0_Th3yhSJg",
        "outputId": "21979c60-0649-4e5c-ce19-ebad298f1471"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1833572387695312"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLING\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size\n",
        "\n",
        "    while True:\n",
        "        emb = Cx[torch.tensor([context])]\n",
        "        h = torch.tanh(emb.view(1, -1) @ w1x + b1x)\n",
        "        logits = h @ w2x + b2x\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "        context = context[1:] + [ix]\n",
        "        if ix ==0:\n",
        "            break\n",
        "        out.append(ix)\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqiG-gL1hW0Y",
        "outputId": "040f036e-cef6-4495-cc52-493a79629da8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora\n",
            "kmyan\n",
            "seel\n",
            "ndhey\n",
            "lorethantendra\n",
            "graderedieli\n",
            "jemi\n",
            "jenleigh\n",
            "esoraan\n",
            "kayzion\n",
            "kalin\n",
            "shuberglairie\n",
            "tricke\n",
            "jeliphetton\n",
            "kubelynder\n",
            "yahulynn\n",
            "yula\n",
            "mustondrihal\n",
            "salynn\n",
            "uhazalel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With uniform initialization, as well, we got almost same loss as random initialization.\n",
        "# Good day, signing off"
      ],
      "metadata": {
        "id": "Zbfc_VvCiXPP"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Q41I7Fzig8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}