{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1IM9VkeMUmiTTLonaY0sAxSVqTm4-fM5G","authorship_tag":"ABX9TyO1S/17lxcV6XymyNvRAO/x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"Ne1LVuJAw5Z5"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"fmzYR2-qoSDf","executionInfo":{"status":"ok","timestamp":1752923254174,"user_tz":-330,"elapsed":71,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["# Collecting Words\n","path = '/content/drive/MyDrive/Colab Notebooks/AndrejKarpathy_NN_Hero/names.txt'\n","words = open(path, 'r').read().splitlines()\n","\n","# Mapping\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}"],"metadata":{"id":"eSmQa88-wAxl","executionInfo":{"status":"ok","timestamp":1752923254214,"user_tz":-330,"elapsed":30,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["block_size = 3 # number of characters as input\n","X, Y = [], []\n","\n","for w in words:\n","    context = [0] * block_size\n","    for ch in w+'.':\n","        ix = stoi[ch]\n","        X.append(context)\n","        Y.append(ix)\n","        context = context[1:] + [ix]\n","\n","X = torch.tensor(X)\n","Y = torch.tensor(Y)\n","\n","# Split the dataset (train: 80%, dev: 10%, test:10%)\n","rs = 2147483647\n","X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, train_size=0.8, random_state=rs, shuffle=True)\n","X_dev, X_test, Y_dev, Y_test = train_test_split(X_temp, Y_temp, train_size=0.5, random_state=rs, shuffle=True)"],"metadata":{"id":"KnAgAg4pwCax","executionInfo":{"status":"ok","timestamp":1752923255928,"user_tz":-330,"elapsed":1711,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["# Backprop Ninja"],"metadata":{"id":"JZ_2CznYw--e"}},{"cell_type":"code","source":["# Utility function to use later: to compare manual gradients with pytorch ones\n","\n","def cmp(s, dt, t):\n","    ex = torch.all(dt == t.grad).item()\n","    app = torch.allclose(dt, t.grad)\n","    maxdiff = (dt-t.grad).abs().max().item()\n","    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"],"metadata":{"id":"ddEz8iE0wDvY","executionInfo":{"status":"ok","timestamp":1752923255966,"user_tz":-330,"elapsed":35,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["n_emb = 10\n","n_hidden = 64\n","vocab_size = 27\n","\n","g = torch.Generator().manual_seed(2147483647)\n","C = torch.randn((vocab_size, n_emb), generator=g)\n","\n","# L1\n","w1 = torch.randn((n_emb * block_size, n_hidden), generator=g) * (5/3)/((n_emb * block_size)**0.5)\n","b1 = torch.randn((n_hidden), generator=g) * 0.01 # biases are useless now as we are using batchnorm layer, using it only for fun\n","\n","# L2\n","w2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n","b2 = torch.randn((vocab_size), generator=g) * 0.1\n","\n","# Batchnorm\n","bngain = torch.randn((1, n_hidden), generator=g) * 0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden), generator=g) * 0.1\n","\n","parameters = [C, w1, b1, w2, b2, bngain, bnbias]\n","for p in parameters:\n","    p.requires_grad = True\n","print(sum(p.nelement() for p in parameters)) # total params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iyEjsDIQywOa","executionInfo":{"status":"ok","timestamp":1752923255998,"user_tz":-330,"elapsed":29,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"outputId":"fc15bf84-8893-44a7-edaf-147793eeeb98"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}]},{"cell_type":"code","source":["# create a mini batch\n","\n","n = batch_size = 32\n","ix = torch.randint(0, X_train.shape[0], (batch_size, ), generator=g)\n","Xb, Yb = X_train[ix], Y_train[ix]"],"metadata":{"id":"VzUT8plg2Ju1","executionInfo":{"status":"ok","timestamp":1752923256178,"user_tz":-330,"elapsed":162,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["<img src='https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimg-blog.csdnimg.cn%2F8ea5b689eb984e51a3d0f5018bc2e7f5.png&f=1&nofb=1&ipt=46bbf325087fa4d91f99379ef97f2672f4761df26ac4f1751f027e5261bdc7d7' height='400px'>"],"metadata":{"id":"fnOE3ZSl7UdZ"}},{"cell_type":"code","source":["# Forward Pass (expanded completely to make manual backprop easier to understand)\n","\n","emb = C[Xb]\n","embcat = emb.view(emb.shape[0], -1) # concatenrate vector columns into a single column\n","eps = 1e-5\n","\n","# Linear 1\n","hprebn = embcat @ w1 + b1 # hidden layer preactivation\n","\n","# Batchnorm\n","bnmeani = (1/n)*hprebn.sum(0, keepdim=True) # mini-batch mean\n","bndiff = hprebn - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = (1/(n-1)) * bndiff2.sum(0, keepdim=True) # mini-batch variance # Bessel's Correction: We are using n-1 instead of n\n","bnvar_inv = (bnvar + eps)**-0.5\n","bnraw = bndiff * bnvar_inv # normalized\n","hpreact = bngain * bnraw + bnbias\n","\n","# Non-linear layer\n","h = torch.tanh(hpreact)\n","\n","# L2\n","logits = h @ w2 + b2 # output layer\n","\n","# Applying cross entory (F.cross_entropy)\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits-logit_maxes # subtract max for numerical stability\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdim=True)\n","counts_sum_inv = counts_sum**-1\n","probs = counts * counts_sum_inv\n","logprobs = probs.log()\n","loss = -logprobs[range(n), Yb].mean()\n","\n","# Pytorch Backward pass\n","for p in parameters:\n","    p.grad = None\n","\n","for t in [logprobs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logit_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiff2, bndiff, bnmeani, hprebn, embcat, emb]:\n","    t.retain_grad()\n","loss.backward()\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MS2Cca3w2_99","executionInfo":{"status":"ok","timestamp":1752923256185,"user_tz":-330,"elapsed":103,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"outputId":"d9c72e80-685b-40d8-f6ba-74188c403b02"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.5251, grad_fn=<NegBackward0>)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## Manual Backprop"],"metadata":{"id":"kdqL5wT8Y6z1"}},{"cell_type":"code","source":["# Ex1: Manual backpropogation of each variable, as defined above\n","# Draw out the flow of variables, will help in understanding the derivatives\n","\n","# d(loss)/d(logprobs)\n","# loss = - (a + b + c)/3\n","# d(loss)/da = -1/3 = -1/n\n","dlogprobs = torch.zeros_like(logprobs) # same shape as logprobs, each element will have it's own gradient\n","# -logprobs[range(n), Yb].mean() - but only some of the elements, 1 from each row is getting selected here, rest will be 0 grad\n","dlogprobs[range(n), Yb] = -1.0/n\n","\n","# dloss/dprobs = d(logprobs)/d(probs) * d(loss)/d(logprobs)\n","dprobs = (1.0 / probs) * dlogprobs\n","\n","# dcounts_sum_inv = dprobs/dcounts_sum_inv * dloss/dprobs\n","dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # to retain the shape of sum_inv\n","dcounts = counts_sum_inv * dprobs\n","\n","dcounts_sum = -1 * counts_sum**-2 * dcounts_sum_inv\n","dcounts += torch.ones_like(counts) * dcounts_sum\n","\n","# dcounts/dnorm_logits * dcounts = norm_logits.exp() * dcounts\n","dnorm_logits = counts * dcounts\n","\n","dlogits = dnorm_logits.clone()\n","dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n","dlogits += F.one_hot(logits.max(1).indices, num_classes= logits.shape[1]) * dlogit_maxes\n","\n","# logits = h @ w2 + b2\n","dh = dlogits @ w2.T\n","dw2 = h.T @ dlogits\n","db2 = dlogits.sum(0)\n","\n","# h = torch.tanh(hpreact)\n","dhpreact = (1.0 - h**2) * dh\n","\n","# hpreact = bngain * bnraw + bnbias\n","dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","dbnraw = (bngain * dhpreact)\n","dbnbias = dhpreact.sum(0, keepdim=True)\n","\n","# bnraw = bndiff * bnvar_inv\n","dbndiff = bnvar_inv * dbnraw\n","dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","\n","# bnvar_inv = (bnvar + eps)**-0.5\n","dbnvar = (-0.5 * (bnvar+eps)**-1.5) * dbnvar_inv\n","\n","# bnvar = (1/(n-1)) * bndiff2.sum(0, keepdim=True)\n","dbndiff2 = ((1.0/(n-1)) * torch.ones_like(bndiff2)) * dbnvar\n","\n","# bndiff2 = bndiff**2\n","dbndiff += (2.0 *bndiff * dbndiff2)\n","\n","# bndiff = hprebn - bnmeani\n","dhprebn = dbndiff.clone()\n","dbnmeani = -dbndiff.sum(0, keepdims=True)\n","\n","# bnmeani = (1/n)*hprebn.sum(0, keepdim=True)\n","dhprebn += ((1/n) * torch.ones_like(hprebn)) * dbnmeani\n","\n","# hprebn = embcat @ w1 + b1\n","dembcat = dhprebn @ w1.T\n","dw1 = embcat.T @ dhprebn\n","db1 = dhprebn.sum(0)\n","\n","# embcat = emb.view(emb.shape[0], -1)\n","demb = dembcat.view(emb.shape)\n","\n","# emb = C[Xb]\n","dC = torch.zeros_like(C)\n","for i in range(Xb.shape[0]):\n","    for j in range(Xb.shape[1]):\n","        ix = Xb[i, j]\n","        dC[ix] += demb[i, j]\n","\n","# Comparing the differences\n","cmp('logprobs', dlogprobs, logprobs)\n","cmp('probs', dprobs, probs)\n","cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n","cmp('counts_sum', dcounts_sum, counts_sum)\n","cmp('counts', dcounts, counts)\n","cmp('norm_logits', dnorm_logits, norm_logits)\n","cmp('logit_maxes', dlogit_maxes, logit_maxes)\n","cmp('logits', dlogits, logits)\n","cmp('h', dh, h)\n","cmp('w2', dw2, w2)\n","cmp('b2', db2, b2)\n","cmp('hpreact', dhpreact, hpreact)\n","cmp('bngain', dbngain, bngain)\n","cmp('bnraw', dbnraw, bnraw)\n","cmp('bnbias', dbnbias, bnbias)\n","cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n","cmp('bnvar', dbnvar, bnvar)\n","cmp('bndiff2', dbndiff2, bndiff2)\n","cmp('bndiff', dbndiff, bndiff)\n","cmp('bnmeani', dbnmeani, bnmeani)\n","cmp('hprebn', dhprebn, hprebn)\n","cmp('embcat', dembcat, embcat)\n","cmp('w1', dw1, w1)\n","cmp('b1', db1, b1)\n","cmp('emb', demb, emb)\n","cmp('C', dC, C)"],"metadata":{"id":"Sbx-qiba1g20","executionInfo":{"status":"ok","timestamp":1752928272926,"user_tz":-330,"elapsed":49,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5749092-d0eb-45a7-8eb2-719b0cb8ce84"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n","probs           | exact: True  | approximate: True  | maxdiff: 0.0\n","counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n","counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n","counts          | exact: True  | approximate: True  | maxdiff: 0.0\n","norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n","logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n","logits          | exact: True  | approximate: True  | maxdiff: 0.0\n","h               | exact: True  | approximate: True  | maxdiff: 0.0\n","w2              | exact: True  | approximate: True  | maxdiff: 0.0\n","b2              | exact: True  | approximate: True  | maxdiff: 0.0\n","hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n","bnvar_inv       | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","bnvar           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bndiff2         | exact: False | approximate: True  | maxdiff: 1.4551915228366852e-11\n","bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","bnmeani         | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n","hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n","embcat          | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\n","w1              | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n","b1              | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n","emb             | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\n","C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"]}]},{"cell_type":"code","source":["# Exercise 4: putting it all together!\n","# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","    p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 200000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","# with torch.no_grad(): # Removed to allow gradient calculation\n","\n","# kick off optimization\n","for i in range(max_steps):\n","\n","    # minibatch construct\n","    ix = torch.randint(0, X_train.shape[0], (batch_size, ), generator=g)\n","    Xb, Yb = X_train[ix], Y_train[ix]\n","    emb = C[Xb]\n","    embcat = emb.view(emb.shape[0], -1) # concatenrate vector columns into a single column\n","    eps = 1e-5\n","\n","    # Linear 1\n","    hprebn = embcat @ W1 + b1 # hidden layer preactivation # Corrected w1 to W1\n","\n","    # Batchnorm\n","    bnmeani = (1/n)*hprebn.sum(0, keepdim=True) # mini-batch mean\n","    bndiff = hprebn - bnmeani\n","    bndiff2 = bndiff**2\n","    bnvar = (1/(n-1)) * bndiff2.sum(0, keepdim=True) # mini-batch variance # Bessel's Correction: We are using n-1 instead of n\n","    bnvar_inv = (bnvar + eps)**-0.5\n","    bnraw = bndiff * bnvar_inv # normalized\n","    hpreact = bngain * bnraw + bnbias\n","\n","    # Non-linear layer\n","    h = torch.tanh(hpreact)\n","\n","    # L2\n","    logits = h @ W2 + b2 # output layer # Corrected w2 to W2\n","\n","    # Applying cross entory (F.cross_entropy)\n","    logit_maxes = logits.max(1, keepdim=True).values\n","    norm_logits = logits-logit_maxes # subtract max for numerical stability\n","    counts = norm_logits.exp()\n","    counts_sum = counts.sum(1, keepdim=True)\n","    counts_sum_inv = counts_sum**-1\n","    probs = counts * counts_sum_inv\n","    logprobs = probs.log()\n","    loss = -logprobs[range(n), Yb].mean()\n","\n","    # backward pass\n","    for p in parameters:\n","        p.grad = None\n","    #loss.backward() # use this for correctness comparisons, delete it later!\n","\n","    # manual backprop! #swole_doge_meme\n","    # -----------------\n","    dlogprobs = torch.zeros_like(logprobs)\n","    dlogprobs[range(n), Yb] = -1.0/n\n","    dprobs = (1.0 / probs) * dlogprobs\n","    dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n","    dcounts = counts_sum_inv * dprobs\n","    dcounts_sum = -1 * counts_sum**-2 * dcounts_sum_inv\n","    dcounts += torch.ones_like(counts) * dcounts_sum\n","    dnorm_logits = counts * dcounts\n","    dlogits = dnorm_logits.clone()\n","    dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n","    dlogits += F.one_hot(logits.max(1).indices, num_classes= logits.shape[1]) * dlogit_maxes # Corrected dlogit_maxes to dlogits_maxes, Typo in original code\n","\n","    dh = dlogits @ W2.T # Corrected w2 to W2\n","    dw2 = h.T @ dlogits # Corrected dw2 to dW2\n","    db2 = dlogits.sum(0)\n","    dhpreact = (1.0 - h**2) * dh\n","    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","    dbnraw = (bngain * dhpreact)\n","    dbnbias = dhpreact.sum(0, keepdim=True)\n","    dbndiff = bnvar_inv * dbnraw\n","    dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n","    dbnvar = (-0.5 * (bnvar+eps)**-1.5) * dbnvar_inv\n","    dbndiff2 = ((1.0/(n-1)) * torch.ones_like(bndiff2)) * dbnvar\n","    dbndiff += (2.0 *bndiff * dbndiff2)\n","    dhprebn = dbndiff.clone()\n","    dbnmeani = -dbndiff.sum(0, keepdims=True)\n","    dhprebn += ((1/n) * torch.ones_like(hprebn)) * dbnmeani\n","\n","    dembcat = dhprebn @ W1.T # Corrected w1 to W1\n","    dw1 = embcat.T @ dhprebn # Corrected dw1 to dW1\n","    db1 = dhprebn.sum(0)\n","    demb = dembcat.view(emb.shape)\n","    dC = torch.zeros_like(C)\n","    for i in range(Xb.shape[0]):\n","        for j in range(Xb.shape[1]):\n","            ix = Xb[i, j]\n","            dC[ix] += demb[i, j]\n","    grads = [dC, dw1, db1, dw2, db2, dbngain, dbnbias] # Corrected dw1, dw2 to dW1, dW2\n","    # -----------------\n","\n","    # update\n","    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n","    for p, grad in zip(parameters, grads):\n","    #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","        p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","    # track stats\n","    if i % 10000 == 0: # print every once in a while\n","        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4gSZVHMqcFw","executionInfo":{"status":"ok","timestamp":1752932814393,"user_tz":-330,"elapsed":1498010,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"outputId":"d242298b-61d1-45c4-975e-671188ceb3ae"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n"]}]},{"cell_type":"code","source":["# calibrate the batch norm at the end of training\n","\n","with torch.no_grad():\n","    # pass the training set through\n","    emb = C[X_train]\n","    embcat = emb.view(emb.shape[0], -1)\n","    hpreact = embcat @ W1 + b1\n","    # measure the mean/std over the entire training set\n","    bnmean = hpreact.mean(0, keepdim=True)\n","    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"],"metadata":{"id":"zjeKGFyA0eML","executionInfo":{"status":"ok","timestamp":1752932815562,"user_tz":-330,"elapsed":1178,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["# DEV & TEST LOSS\n","\n","@torch.no_grad() # decorator to disable grad tracking for any vars inside the function\n","def split_loss(split):\n","    x, y = {\n","        'train': (X_train, Y_train),\n","        'val': (X_dev, Y_dev),\n","        'test': (X_test, Y_test)\n","    }[split]\n","\n","    emb = C[x] # (N, block_size, n_embd)\n","    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","    hpreact = embcat @ W1 + b1\n","    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","    h = torch.tanh(hpreact) # (N, n_hidden)\n","    logits = h @ W2 + b2 # (N, vocab_size)\n","    loss = F.cross_entropy(logits, y)\n","    print(split, loss.item())\n","\n","split_loss('val')\n","split_loss('test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55cVKtnH0vpZ","executionInfo":{"status":"ok","timestamp":1752932815801,"user_tz":-330,"elapsed":235,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"outputId":"6147f001-ba94-4193-d6f0-133d8bf30ec1"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["val 2.178706645965576\n","test 2.177915096282959\n"]}]},{"cell_type":"code","source":["g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","\n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","        # ------------\n","        # forward pass:\n","        # Embedding\n","        emb = C[torch.tensor([context])] # (1,block_size,d)\n","        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","        hpreact = embcat @ W1 + b1\n","        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","        h = torch.tanh(hpreact) # (N, n_hidden)\n","        logits = h @ W2 + b2 # (N, vocab_size)\n","        # ------------\n","        # Sample\n","        probs = F.softmax(logits, dim=1)\n","        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","        context = context[1:] + [ix]\n","        out.append(ix)\n","        if ix == 0:\n","            break\n","\n","    print(''.join(itos[i] for i in out))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tm-U6pjI08SO","executionInfo":{"status":"ok","timestamp":1752932815869,"user_tz":-330,"elapsed":61,"user":{"displayName":"Shivanand P","userId":"06993734202145475976"}},"outputId":"54303333-d1d1-497c-90c7-8803489eb9f3"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["mora.\n","gryanna.\n","elmadhayla.\n","redhasiendradge.\n","zehed.\n","elin.\n","shyonelle.\n","elieananaraelynn.\n","hokelin.\n","shervredhira.\n","sten.\n","joselynn.\n","novanna.\n","brence.\n","ruy.\n","julieh.\n","yuma.\n","maston.\n","azhianna.\n","yansunazalel.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GNeUhaFa2alT"},"execution_count":null,"outputs":[]}]}